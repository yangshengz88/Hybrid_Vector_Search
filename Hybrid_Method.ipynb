{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e116a65d-f25a-41c2-8c78-13d24e14f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b13a5cc-d5ec-4c1e-b9cc-cf32d5e9ebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Filter Helper Class, it can also be imported from metadata_indexing file\n",
    "class MetadataFilter:\n",
    "    \"\"\"Helper class for efficient metadata filtering\"\"\"\n",
    "    \n",
    "    def __init__(self, categorical_indexes, numeric_indexes, n_samples):\n",
    "        self.categorical_indexes = categorical_indexes\n",
    "        self.numeric_indexes = numeric_indexes\n",
    "        self.n_samples = n_samples\n",
    "    \n",
    "    def apply_filter(self, filter_conditions):\n",
    "        \"\"\"\n",
    "        Apply filter conditions and return a boolean bitmap\n",
    "        \n",
    "        filter_conditions format:\n",
    "        {\n",
    "            'artist': ['Coldplay', 'Radiohead'],\n",
    "            'year': (2000, 2020),\n",
    "            'tempo': (80, 150),\n",
    "            'mode': [1]\n",
    "        }\n",
    "        \"\"\"\n",
    "        # Start with all True\n",
    "        bitmap = np.ones(self.n_samples, dtype=bool)\n",
    "        \n",
    "        for key, value in filter_conditions.items():\n",
    "            if key in self.categorical_indexes:\n",
    "                # Categorical filter\n",
    "                valid_indices = set()\n",
    "                if isinstance(value, list):\n",
    "                    for val in value:\n",
    "                        if val in self.categorical_indexes[key]:\n",
    "                            valid_indices.update(self.categorical_indexes[key][val])\n",
    "                else:\n",
    "                    if value in self.categorical_indexes[key]:\n",
    "                        valid_indices.update(self.categorical_indexes[key][value])\n",
    "                \n",
    "                temp_bitmap = np.zeros(self.n_samples, dtype=bool)\n",
    "                temp_bitmap[list(valid_indices)] = True\n",
    "                bitmap &= temp_bitmap\n",
    "            \n",
    "            elif key in self.numeric_indexes:\n",
    "                # Numeric range filter\n",
    "                if isinstance(value, tuple) and len(value) == 2:\n",
    "                    min_val, max_val = value\n",
    "                    sorted_indices = np.array(self.numeric_indexes[key]['sorted_indices'])\n",
    "                    sorted_values = np.array(self.numeric_indexes[key]['sorted_values'])\n",
    "                    \n",
    "                    valid_indices = indices_in_range(sorted_indices, sorted_values, min_val, max_val)\n",
    "                    \n",
    "                    temp_bitmap = np.zeros(self.n_samples, dtype=bool)\n",
    "                    temp_bitmap[valid_indices] = True\n",
    "                    bitmap &= temp_bitmap\n",
    "        \n",
    "        return bitmap\n",
    "    \n",
    "    def get_valid_indices(self, filter_conditions):\n",
    "        \"\"\"Return list of valid indices after filtering\"\"\"\n",
    "        bitmap = self.apply_filter(filter_conditions)\n",
    "        return np.where(bitmap)[0].tolist()\n",
    "    \n",
    "    def get_selectivity(self, filter_conditions):\n",
    "        \"\"\"Return selectivity (fraction of data passing filter)\"\"\"\n",
    "        bitmap = self.apply_filter(filter_conditions)\n",
    "        return bitmap.sum() / self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58b3d6e7-283d-485d-b3ac-6cb27e5b815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define indice_in_range, it can also be imported from metadata_indexing file\n",
    "def indices_in_range(sorted_indices, sorted_values, min_val, max_val):\n",
    "    \"\"\"Binary search to find indices within a range\"\"\"\n",
    "    left = np.searchsorted(sorted_values, min_val, side='left')\n",
    "    right = np.searchsorted(sorted_values, max_val, side='right')\n",
    "    return sorted_indices[left:right].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db2a9b60-24fe-4537-8813-f866b69ac675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 169776 embeddings\n",
      "IVF index with 412 clusters\n"
     ]
    }
   ],
   "source": [
    "# Load Required Data\n",
    "# ============================================\n",
    "print(\"Loading data...\")\n",
    "embeddings = np.load(\"data/spotify_vectors_10d.npy\").astype('float32')\n",
    "index_ivf = faiss.read_index(\"data/index_ivf_flat.faiss\")\n",
    "index_flat = faiss.read_index(\"data/index_flat_l2.faiss\")\n",
    "\n",
    "with open(\"data/metadata_filter.pkl\", \"rb\") as f:\n",
    "    metadata_filter = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {embeddings.shape[0]} embeddings\")\n",
    "print(f\"IVF index with {index_ivf.nlist} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "646da939-5087-46b4-b306-87e110382a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Precomputing Cluster Metadata ===\n",
      "Computed cluster assignments for 412 clusters\n"
     ]
    }
   ],
   "source": [
    "# 1. Cluster-Level Metadata Precomputation\n",
    "# ============================================\n",
    "print(\"\\n=== Precomputing Cluster Metadata ===\")\n",
    "\n",
    "def compute_cluster_assignments(index_ivf, embeddings):\n",
    "    \"\"\"Compute which cluster each vector belongs to\"\"\"\n",
    "    quantizer = faiss.downcast_index(index_ivf.quantizer)\n",
    "    _, cluster_ids = quantizer.search(embeddings, 1)\n",
    "    return cluster_ids.flatten()\n",
    "\n",
    "cluster_assignments = compute_cluster_assignments(index_ivf, embeddings)\n",
    "\n",
    "# Build cluster -> track indices mapping\n",
    "cluster_to_tracks = defaultdict(list)\n",
    "for track_idx, cluster_id in enumerate(cluster_assignments):\n",
    "    cluster_to_tracks[cluster_id].append(track_idx)\n",
    "\n",
    "print(f\"Computed cluster assignments for {len(cluster_to_tracks)} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64457014-6703-434c-bc96-330d3220bba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Build Cluster-Level Filter Bitmaps\n",
    "# ============================================\n",
    "\n",
    "def build_cluster_filter_bitmaps(cluster_to_tracks, metadata_filter, filter_conditions):\n",
    "    \"\"\"\n",
    "    For each cluster, compute a bitmap indicating whether it contains\n",
    "    any tracks that satisfy the filter conditions.\n",
    "    \n",
    "    Returns:\n",
    "        cluster_has_valid_tracks: dict mapping cluster_id -> bool\n",
    "        tracks_per_cluster_passing: dict mapping cluster_id -> list of valid track indices\n",
    "    \"\"\"\n",
    "    cluster_has_valid_tracks = {}\n",
    "    tracks_per_cluster_passing = {}\n",
    "    \n",
    "    # Get global valid tracks bitmap\n",
    "    global_valid_bitmap = metadata_filter.apply_filter(filter_conditions)\n",
    "    \n",
    "    for cluster_id, track_indices in cluster_to_tracks.items():\n",
    "        # Check which tracks in this cluster pass the filter\n",
    "        valid_tracks_in_cluster = [idx for idx in track_indices if global_valid_bitmap[idx]]\n",
    "        \n",
    "        cluster_has_valid_tracks[cluster_id] = len(valid_tracks_in_cluster) > 0\n",
    "        tracks_per_cluster_passing[cluster_id] = valid_tracks_in_cluster\n",
    "    \n",
    "    return cluster_has_valid_tracks, tracks_per_cluster_passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3456440-34ec-41b2-9a41-152ef5e98071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Hybrid Search with Predicate Pushdown\n",
    "# ============================================\n",
    "\n",
    "class HybridSearcher:\n",
    "    \"\"\"\n",
    "    Hybrid search that integrates metadata filtering with FAISS traversal\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, index_ivf, index_flat, embeddings, cluster_to_tracks, metadata_filter):\n",
    "        self.index_ivf = index_ivf\n",
    "        self.index_flat = index_flat\n",
    "        self.embeddings = embeddings\n",
    "        self.cluster_to_tracks = cluster_to_tracks\n",
    "        self.metadata_filter = metadata_filter\n",
    "        self.n_samples = len(embeddings)\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'total_queries': 0,\n",
    "            'clusters_probed': 0,\n",
    "            'clusters_skipped': 0,\n",
    "            'distance_computations': 0,\n",
    "            'total_latency': 0.0\n",
    "        }\n",
    "    \n",
    "    def search_with_predicate_pushdown(self, query, k, filter_conditions, nprobe=20):\n",
    "        \"\"\"\n",
    "        Perform hybrid search with predicate pushdown\n",
    "        \n",
    "        Args:\n",
    "            query: Query embedding (1, d)\n",
    "            k: Number of results to return\n",
    "            filter_conditions: Metadata filter dict\n",
    "            nprobe: Number of clusters to probe\n",
    "            \n",
    "        Returns:\n",
    "            distances: Top-k distances\n",
    "            indices: Top-k track indices\n",
    "            stats: Search statistics\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Build cluster-level filter bitmaps\n",
    "        cluster_has_valid, tracks_per_cluster = build_cluster_filter_bitmaps(\n",
    "            self.cluster_to_tracks, self.metadata_filter, filter_conditions\n",
    "        )\n",
    "        \n",
    "        # Find nearest clusters\n",
    "        quantizer = faiss.downcast_index(self.index_ivf.quantizer)\n",
    "        _, nearest_clusters = quantizer.search(query, nprobe * 2)  # Get more candidates\n",
    "        nearest_clusters = nearest_clusters.flatten()\n",
    "        \n",
    "        # Collect candidates with predicate pushdown\n",
    "        candidates = []\n",
    "        clusters_probed = 0\n",
    "        clusters_skipped = 0\n",
    "        distance_computations = 0\n",
    "        \n",
    "        for cluster_id in nearest_clusters:\n",
    "            cluster_id = int(cluster_id)\n",
    "            \n",
    "            # Predicate pushdown: skip clusters without valid tracks\n",
    "            if cluster_id not in cluster_has_valid or not cluster_has_valid[cluster_id]:\n",
    "                clusters_skipped += 1\n",
    "                continue\n",
    "            \n",
    "            clusters_probed += 1\n",
    "            \n",
    "            # Get valid tracks from this cluster\n",
    "            valid_tracks = tracks_per_cluster[cluster_id]\n",
    "            \n",
    "            # Compute distances only for valid tracks\n",
    "            for track_idx in valid_tracks:\n",
    "                dist = np.linalg.norm(query[0] - self.embeddings[track_idx])\n",
    "                candidates.append((dist, track_idx))\n",
    "                distance_computations += 1\n",
    "            \n",
    "            # Early stopping if we have enough candidates\n",
    "            if len(candidates) >= k * 3:\n",
    "                break\n",
    "        \n",
    "        # Sort and get top-k\n",
    "        candidates.sort(key=lambda x: x[0])\n",
    "        top_k = candidates[:k]\n",
    "        \n",
    "        if len(top_k) < k:\n",
    "            # Pad with dummy results if not enough valid results\n",
    "            while len(top_k) < k:\n",
    "                top_k.append((float('inf'), -1))\n",
    "        \n",
    "        distances = np.array([d for d, _ in top_k])\n",
    "        indices = np.array([idx for _, idx in top_k])\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        # Update statistics\n",
    "        self.stats['total_queries'] += 1\n",
    "        self.stats['clusters_probed'] += clusters_probed\n",
    "        self.stats['clusters_skipped'] += clusters_skipped\n",
    "        self.stats['distance_computations'] += distance_computations\n",
    "        self.stats['total_latency'] += latency\n",
    "        \n",
    "        search_stats = {\n",
    "            'latency': latency,\n",
    "            'clusters_probed': clusters_probed,\n",
    "            'clusters_skipped': clusters_skipped,\n",
    "            'distance_computations': distance_computations,\n",
    "            'candidates_found': len(candidates),\n",
    "            'nprobe_requested': nprobe\n",
    "        }\n",
    "        \n",
    "        return distances, indices, search_stats\n",
    "    \n",
    "    def search_baseline_postfilter(self, query, k, filter_conditions, nprobe=20, candidate_multiplier=10):\n",
    "        \"\"\"\n",
    "        Baseline: Retrieve large candidate pool, then post-filter\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Retrieve more candidates\n",
    "        self.index_ivf.nprobe = nprobe\n",
    "        k_candidates = k * candidate_multiplier\n",
    "        D, I = self.index_ivf.search(query, k_candidates)\n",
    "        \n",
    "        # Apply metadata filter\n",
    "        valid_bitmap = self.metadata_filter.apply_filter(filter_conditions)\n",
    "        \n",
    "        # Filter candidates\n",
    "        valid_results = []\n",
    "        for dist, idx in zip(D[0], I[0]):\n",
    "            if idx >= 0 and valid_bitmap[idx]:\n",
    "                valid_results.append((dist, idx))\n",
    "                if len(valid_results) >= k:\n",
    "                    break\n",
    "        \n",
    "        # Pad if needed\n",
    "        while len(valid_results) < k:\n",
    "            valid_results.append((float('inf'), -1))\n",
    "        \n",
    "        distances = np.array([d for d, _ in valid_results])\n",
    "        indices = np.array([idx for _, idx in valid_results])\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        stats = {\n",
    "            'latency': latency,\n",
    "            'candidates_retrieved': k_candidates,\n",
    "            'distance_computations': k_candidates,  # All candidates computed\n",
    "            'valid_found': len([idx for idx in indices if idx >= 0])\n",
    "        }\n",
    "        \n",
    "        return distances, indices, stats\n",
    "    \n",
    "    def search_baseline_prefilter(self, query, k, filter_conditions):\n",
    "        \"\"\"\n",
    "        Baseline: Pre-filter, then exact search on filtered subset\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get valid indices\n",
    "        valid_indices = self.metadata_filter.get_valid_indices(filter_conditions)\n",
    "        \n",
    "        if len(valid_indices) == 0:\n",
    "            # No valid tracks\n",
    "            return np.full(k, float('inf')), np.full(k, -1), {'latency': time.time() - start_time}\n",
    "        \n",
    "        # Extract valid embeddings\n",
    "        valid_embeddings = self.embeddings[valid_indices]\n",
    "        \n",
    "        # Compute distances\n",
    "        distances_all = np.linalg.norm(valid_embeddings - query[0], axis=1)\n",
    "        \n",
    "        # Get top-k\n",
    "        if len(distances_all) < k:\n",
    "            top_k_local = np.arange(len(distances_all))\n",
    "            padding_needed = k - len(distances_all)\n",
    "            distances = np.concatenate([distances_all, np.full(padding_needed, float('inf'))])\n",
    "            indices = np.concatenate([np.array(valid_indices), np.full(padding_needed, -1)])\n",
    "        else:\n",
    "            top_k_local = np.argpartition(distances_all, k)[:k]\n",
    "            top_k_local = top_k_local[np.argsort(distances_all[top_k_local])]\n",
    "            distances = distances_all[top_k_local]\n",
    "            indices = np.array(valid_indices)[top_k_local]\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        stats = {\n",
    "            'latency': latency,\n",
    "            'filtered_size': len(valid_indices),\n",
    "            'distance_computations': len(valid_indices)\n",
    "        }\n",
    "        \n",
    "        return distances, indices, stats\n",
    "    \n",
    "    def get_ground_truth(self, query, k, filter_conditions):\n",
    "        \"\"\"Compute ground truth using brute force on filtered data\"\"\"\n",
    "        valid_indices = self.metadata_filter.get_valid_indices(filter_conditions)\n",
    "        \n",
    "        if len(valid_indices) == 0:\n",
    "            return np.full(k, float('inf')), np.full(k, -1)\n",
    "        \n",
    "        valid_embeddings = self.embeddings[valid_indices]\n",
    "        distances_all = np.linalg.norm(valid_embeddings - query[0], axis=1)\n",
    "        \n",
    "        if len(distances_all) < k:\n",
    "            top_k_local = np.arange(len(distances_all))\n",
    "            distances = np.concatenate([distances_all, np.full(k - len(distances_all), float('inf'))])\n",
    "            indices = np.concatenate([np.array(valid_indices), np.full(k - len(distances_all), -1)])\n",
    "        else:\n",
    "            top_k_local = np.argpartition(distances_all, k)[:k]\n",
    "            top_k_local = top_k_local[np.argsort(distances_all[top_k_local])]\n",
    "            distances = distances_all[top_k_local]\n",
    "            indices = np.array(valid_indices)[top_k_local]\n",
    "        \n",
    "        return distances, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b1c9224-ce42-4d13-9a6d-4d55894a139a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Initializing Hybrid Searcher ===\n",
      "\n",
      "=== Testing with filter: {'year': (2010, 2020), 'mode': [1]} ===\n",
      "Filter selectivity: 8.21%\n",
      "\n",
      "1. Hybrid Search (Predicate Pushdown)\n",
      "   Latency: 53.70ms\n",
      "   Clusters probed: 7\n",
      "   Clusters skipped: 12\n",
      "   Distance computations: 64\n",
      "\n",
      "2. Post-Filter Baseline\n",
      "   Latency: 32.41ms\n",
      "   Distance computations: 100\n",
      "   Valid results found: 0\n",
      "\n",
      "3. Pre-Filter Baseline\n",
      "   Latency: 27.63ms\n",
      "   Distance computations: 13940\n",
      "\n",
      "4. Ground Truth (Brute Force)\n",
      "   Latency: 33.25ms\n",
      "\n",
      "=== Recall@10 ===\n",
      "Hybrid (Predicate Pushdown): 0.700\n",
      "Post-Filter Baseline: 0.000\n",
      "Pre-Filter Baseline: 1.000\n",
      "\n",
      "=== Speedup Analysis ===\n",
      "Hybrid vs Post-Filter: 0.60x faster\n",
      "Hybrid vs Pre-Filter: 0.51x\n",
      "Distance computation reduction: 36.0%\n",
      "\n",
      "=== Hybrid Method Implementation Complete ===\n",
      "Saved hybrid searcher to data/hybrid_searcher.pkl\n"
     ]
    }
   ],
   "source": [
    "# 4. Example Usage and Testing\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== Initializing Hybrid Searcher ===\")\n",
    "hybrid_searcher = HybridSearcher(\n",
    "    index_ivf, index_flat, embeddings, \n",
    "    cluster_to_tracks, metadata_filter\n",
    ")\n",
    "\n",
    "# Test query\n",
    "test_query = embeddings[5000:5001]  # Use a track from middle of dataset\n",
    "k = 10\n",
    "\n",
    "# Example filter - create realistic filter based on available data\n",
    "test_filter = {}\n",
    "\n",
    "# Add filters based on what's available\n",
    "if hasattr(metadata_filter, 'numeric_indexes') and 'year' in metadata_filter.numeric_indexes:\n",
    "    # Use a realistic year range\n",
    "    test_filter['year'] = (2010, 2020)\n",
    "\n",
    "if hasattr(metadata_filter, 'categorical_indexes') and 'mode' in metadata_filter.categorical_indexes:\n",
    "    test_filter['mode'] = [1]  # Major key\n",
    "\n",
    "# If no filters available, use a simple one\n",
    "if not test_filter:\n",
    "    print(\"Warning: Using minimal filter for testing\")\n",
    "    test_filter = {'mode': [1]} if 'mode' in metadata_filter.categorical_indexes else {}\n",
    "\n",
    "print(f\"\\n=== Testing with filter: {test_filter} ===\")\n",
    "print(f\"Filter selectivity: {metadata_filter.get_selectivity(test_filter)*100:.2f}%\")\n",
    "\n",
    "# Run all three methods\n",
    "print(\"\\n1. Hybrid Search (Predicate Pushdown)\")\n",
    "D_hybrid, I_hybrid, stats_hybrid = hybrid_searcher.search_with_predicate_pushdown(\n",
    "    test_query, k, test_filter, nprobe=20\n",
    ")\n",
    "print(f\"   Latency: {stats_hybrid['latency']*1000:.2f}ms\")\n",
    "print(f\"   Clusters probed: {stats_hybrid['clusters_probed']}\")\n",
    "print(f\"   Clusters skipped: {stats_hybrid['clusters_skipped']}\")\n",
    "print(f\"   Distance computations: {stats_hybrid['distance_computations']}\")\n",
    "\n",
    "print(\"\\n2. Post-Filter Baseline\")\n",
    "D_post, I_post, stats_post = hybrid_searcher.search_baseline_postfilter(\n",
    "    test_query, k, test_filter, nprobe=20\n",
    ")\n",
    "print(f\"   Latency: {stats_post['latency']*1000:.2f}ms\")\n",
    "print(f\"   Distance computations: {stats_post['distance_computations']}\")\n",
    "print(f\"   Valid results found: {stats_post['valid_found']}\")\n",
    "\n",
    "print(\"\\n3. Pre-Filter Baseline\")\n",
    "D_pre, I_pre, stats_pre = hybrid_searcher.search_baseline_prefilter(\n",
    "    test_query, k, test_filter\n",
    ")\n",
    "print(f\"   Latency: {stats_pre['latency']*1000:.2f}ms\")\n",
    "print(f\"   Distance computations: {stats_pre['distance_computations']}\")\n",
    "\n",
    "# Ground truth\n",
    "print(\"\\n4. Ground Truth (Brute Force)\")\n",
    "start = time.time()\n",
    "D_true, I_true = hybrid_searcher.get_ground_truth(test_query, k, test_filter)\n",
    "gt_latency = time.time() - start\n",
    "print(f\"   Latency: {gt_latency*1000:.2f}ms\")\n",
    "\n",
    "# Calculate recalls\n",
    "def calculate_recall(retrieved, ground_truth, k):\n",
    "    retrieved_set = set(retrieved[:k][retrieved[:k] >= 0])\n",
    "    gt_set = set(ground_truth[:k][ground_truth[:k] >= 0])\n",
    "    if len(gt_set) == 0:\n",
    "        return 0.0\n",
    "    return len(retrieved_set & gt_set) / len(gt_set)\n",
    "\n",
    "recall_hybrid = calculate_recall(I_hybrid, I_true, k)\n",
    "recall_post = calculate_recall(I_post, I_true, k)\n",
    "recall_pre = calculate_recall(I_pre, I_true, k)\n",
    "\n",
    "print(f\"\\n=== Recall@{k} ===\")\n",
    "print(f\"Hybrid (Predicate Pushdown): {recall_hybrid:.3f}\")\n",
    "print(f\"Post-Filter Baseline: {recall_post:.3f}\")\n",
    "print(f\"Pre-Filter Baseline: {recall_pre:.3f}\")\n",
    "\n",
    "print(\"\\n=== Speedup Analysis ===\")\n",
    "print(f\"Hybrid vs Post-Filter: {stats_post['latency']/stats_hybrid['latency']:.2f}x faster\")\n",
    "print(f\"Hybrid vs Pre-Filter: {stats_pre['latency']/stats_hybrid['latency']:.2f}x\")\n",
    "print(f\"Distance computation reduction: {(1 - stats_hybrid['distance_computations']/stats_post['distance_computations'])*100:.1f}%\")\n",
    "\n",
    "# Save the hybrid searcher\n",
    "with open(\"data/hybrid_searcher.pkl\", \"wb\") as f:\n",
    "    pickle.dump(hybrid_searcher, f)\n",
    "\n",
    "print(\"\\n=== Hybrid Method Implementation Complete ===\")\n",
    "print(\"Saved hybrid searcher to data/hybrid_searcher.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed451ddc-7d49-4c03-988f-cd7e29ec1231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d770126b-d31d-4739-8f00-17a723493ece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
